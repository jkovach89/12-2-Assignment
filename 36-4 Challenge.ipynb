{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Python3\n",
    "# Credit to Rumperuu - https://github.com/Rumperuu/\n",
    "\n",
    "import scrapy, os\n",
    "from enum import Enum\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-06 19:00:27 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2020-01-06 19:00:27 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug 13 2019, 15:17:50) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Darwin-19.0.0-x86_64-i386-64bit\n",
      "2020-01-06 19:00:27 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2020-01-06 19:00:27 [scrapy.extensions.telnet] INFO: Telnet Password: 0dbd2865642ae611\n",
      "2020-01-06 19:00:27 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-01-06 19:00:27 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-01-06 19:00:27 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-01-06 19:00:27 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-01-06 19:00:27 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-01-06 19:00:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-01-06 19:00:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-01-06 19:00:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://seekingalpha.com/earnings/earnings-call-transcripts/1> from <GET http://seekingalpha.com/earnings/earnings-call-transcripts/1>\n",
      "2020-01-06 19:00:27 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://seekingalpha.com/earnings/earnings-call-transcripts/1> (referer: None)\n",
      "2020-01-06 19:00:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://seekingalpha.com/earnings/earnings-call-transcripts/1>: HTTP status code is not handled or not allowed\n",
      "2020-01-06 19:00:27 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-01-06 19:00:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 540,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 952,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 1, 7, 3, 0, 27, 919691),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 72822784,\n",
      " 'memusage/startup': 72822784,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2020, 1, 7, 3, 0, 27, 609361)}\n",
      "2020-01-06 19:00:27 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "Stage = Enum('Stage', 'preamble execs analysts body')\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "transcripts = {}\n",
    "\n",
    "class AlphaSpider(scrapy.Spider):\n",
    "\n",
    "    name = \"transcripts\"\n",
    "    start_urls = [\"http://seekingalpha.com/earnings/earnings-call-transcripts/1\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Follow each transcript page link from the index page\n",
    "        for href in response.css('.dashboard-article-link::attr(href)').extract():\n",
    "            yield scrapy.Request(response.urljoin(href), callback=self.parse_transcript)\n",
    "            \n",
    "        # Follows pagination links at the bottom of index page\n",
    "        next_page = response.css('li.next a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "    def parse_transcript(self, response):\n",
    "        i = 0\n",
    "        transcript = {}\n",
    "        details = {}\n",
    "        execs = []\n",
    "        analysts = []\n",
    "        script = []\n",
    "        mode = 1\n",
    "        \n",
    "        # Pages are represented by a series of <p> elements, all with the same '.p1' class and no unique \n",
    "        # identfiers, we have to breaking it into chunks and iterate over them.\n",
    "        body = response.css('div#a-body p.p1')\n",
    "        chunks = body.css('p.p1')\n",
    "        while i < len(chunks):\n",
    "            # If the current line is a heading and we're not currently going\n",
    "            # through the transcript body (where headings represent speakers),\n",
    "            # change the current section flag to the next section.\n",
    "            if (len(chunks[i].css('strong::text').extract())==0) or (mode==4):\n",
    "                currStage = Stage(mode)\n",
    "                if currStage == Stage['preamble']:\n",
    "                    if i == 0:\n",
    "                        if len(chunks[1].css('strong::text').extract()) == 0:\n",
    "                            details['company'] = chunks[i].css('p::text').extract_first()\n",
    "                            if \" (\" in details['company']:\n",
    "                                details['company'] = details['company'].split(' (')[0]\n",
    "                            details['exchange'] = \"NYSE\"\n",
    "                            details['ticker'] = chunks.css('a::text').extract_first()\n",
    "                            if \":\" in details['ticker']:\n",
    "                                ticker = details['ticker'].split(':')\n",
    "                                details['exchange'] = ticker[0]\n",
    "                                details['ticker'] = ticker[1]\n",
    "                        else:\n",
    "                            details['company'] = chunks[i].css('p::text').extract_first()\n",
    "                            if \" (\" in details['company']:\n",
    "                                details['company'] = details['company'].split(' (')[0]\n",
    "                            # if a specific stock exchange is not listed, default to NYSE\n",
    "                            details['exchange'] = \"NYSE\"\n",
    "                            details['ticker'] = chunks.css('a::text').extract_first()\n",
    "                            if \":\" in details['ticker']:\n",
    "                                ticker = details['ticker'].split(':')\n",
    "                                details['exchange'] = ticker[0]\n",
    "                                details['ticker'] = ticker[1]\n",
    "                            titleAndDate = chunks[i].css('p::text').extract[1]\n",
    "                            for date in months:\n",
    "                                if date in titleAndDate:\n",
    "                                    splits = titleAndDate.split(date)\n",
    "                                    details['title'] = splits[0]\n",
    "                                    details['date'] = date + splits[1]\n",
    "                    # Otherwise, we're onto the title line.\n",
    "                    elif i == 1:\n",
    "                        title = chunks[i].css('p::text').extract_first()\n",
    "                        # This should never be the case, but just to be careful I'm leaving it in.\n",
    "                        if len(title) <= 0:\n",
    "                            title = \"NO TITLE\"\n",
    "                        details['title'] = title\n",
    "                    # Or the date line.\n",
    "                    elif i == 2:\n",
    "                        details['date'] = chunks[i].css('p::text').extract_first()\n",
    "                # If we're onto the 'Executives' section, we create a list of\n",
    "                # all of their names, positions and company name (from the \n",
    "                # preamble).\n",
    "                elif currStage == Stage['execs']:\n",
    "                    anExec = chunks[i].css('p::text').extract_first().split(\" - \")\n",
    "                    # This covers if the execs are separated with an em- rather\n",
    "                    # than an en-dash (see above).\n",
    "                    if len(anExec) <= 1:\n",
    "                        anExec = chunks[i].css('p::text').extract_first().split(\" – \")\n",
    "                    name = anExec[0]\n",
    "                    if len(anExec) > 1:\n",
    "                        position = anExec[1]\n",
    "                    # Again, this should never be the case, as an Exec-less\n",
    "                    # company would find it hard to get much done.\n",
    "                    else:\n",
    "                        position = \"\"\n",
    "                    execs.append((name,position,details['company']))\n",
    "                # This does the same, but with the analysts (which never seem\n",
    "                # to be separated by em-dashes for some reason).\n",
    "                elif currStage == Stage['analysts']:\n",
    "                    name = chunks[i].css('p::text').extract_first().split(\" - \")[0]\n",
    "                    company = chunks[i].css('p::text').extract_first().split(\" - \")[1]\n",
    "                    analysts.append((name,company))\n",
    "                # This strips the transcript body of everything except simple\n",
    "                # HTML, and stores that.\n",
    "                elif currStage == Stage['body']:\n",
    "                    line = chunks[i].css('p::text').extract_first()\n",
    "                    html = \"p>\"\n",
    "                    if line is None:\n",
    "                        line = chunks[i].css('strong::text').extract_first()\n",
    "                        html = \"h1>\"\n",
    "                    script.append(\"<\"+html+line+\"</\"+html)\n",
    "            else:\n",
    "                mode += 1\n",
    "            i += 1\n",
    "\n",
    "        # Adds the various arrays to the dictionary for the transcript\n",
    "        details['exec'] = execs \n",
    "        details['analysts'] = analysts\n",
    "        details['transcript'] = ''.join(script)\n",
    "\n",
    "        # Adds this transcript to the dictionary of all scraped\n",
    "        # transcripts, and yield that for the output\n",
    "        transcript[\"entry\"] = details\n",
    "        yield transcript\n",
    "\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(AlphaSpider)\n",
    "process.start()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('PythonLinks.json')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API to pull twitter data\n",
    "import scrapy, pandas as pd\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.http import FormRequest\n",
    "\n",
    "class AlphaSpider(scrapy.Spider):\n",
    "    name='AS'\n",
    "    allowed_domains = [\"seekingalpha.com\"]\n",
    "\n",
    "    start_urls = [\n",
    "        'https://jkovach89:seekingalpha.com/earnings/earnings-call-transcripts'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    BASE_URL = 'https://seekingalpha.com/'\n",
    "    \n",
    "    def parse(self, response):\n",
    "        token = response.xpath('//*[@name=\"csrf-token\"]/@value').extractfirst()\n",
    "        \n",
    "        return FormRequest.from_response(response, \n",
    "                                         form_data{\n",
    "                                             'csrf_token'=token,\n",
    "                                             'password'='foobar',\n",
    "                                             'user_name'='foobar'\n",
    "                                         }, callback_self=scrape_pages)\n",
    "        \n",
    "        links = response.xpath('//a[@class=\"hdrlnk\"]/@href').extract()\n",
    "        for link in links:\n",
    "            absolute_url = self.BASE_URL + link\n",
    "            yield scrapy.Request(absolute_url, callback=self.parse_attr)\n",
    "        \n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "    \n",
    "        \n",
    "    def parse_attr(self, response):\n",
    "        item = DmozItem()\n",
    "        item[\"link\"] = response.url\n",
    "        item[\"attr\"] = \"\".join(response.xpath(\"//p[@class='attrgroup']//text()\").extract())\n",
    "        return item\n",
    "    \n",
    "    def parse(self,response):\n",
    "        with open('transcripts.html', 'wb') as file:\n",
    "            file.write(response.body)\n",
    "    \n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(AlphaSpider)\n",
    "process.start()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "# item class included here \n",
    "class DmozItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    link = scrapy.Field()\n",
    "    attr = scrapy.Field()\n",
    "\n",
    "\n",
    "class DmozSpider(scrapy.Spider):\n",
    "    name = \"dmoz\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "    \"http://chicago.craigslist.org/search/emd?\"\n",
    "    ]\n",
    "\n",
    "    BASE_URL = 'http://chicago.craigslist.org/'\n",
    "\n",
    "    def parse(self, response):\n",
    "        links = response.xpath('//a[@class=\"hdrlnk\"]/@href').extract()\n",
    "        for link in links:\n",
    "            absolute_url = self.BASE_URL + link\n",
    "            yield scrapy.Request(absolute_url, callback=self.parse_attr)\n",
    "\n",
    "    def parse_attr(self, response):\n",
    "        item = DmozItem()\n",
    "        item[\"link\"] = response.url\n",
    "        item[\"attr\"] = \"\".join(response.xpath(\"//p[@class='attrgroup']//text()\").extract())\n",
    "        return item\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
