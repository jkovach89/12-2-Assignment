{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning/processing/language parsing\n",
    "import pandas as pd, spacy, nltk, re\n",
    "from nltk.corpus import twitter_samples\n",
    "from collections import Counter\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "print(twitter_samples.fileids())\n",
    "neg = twitter_samples.strings('negative_tweets.json')\n",
    "pos = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(for, being, top, engaged, members, in, my, co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(Hey, James, !, How, odd, Please, call, our, C...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(we, had, a, listen, last, night, As, You, Ble...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(CONGRATS)</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(yeaaaah, yippppy, !, !, !, my, accnt, verifie...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  (for, being, top, engaged, members, in, my, co...  Positive\n",
       "1  (Hey, James, !, How, odd, Please, call, our, C...  Positive\n",
       "2  (we, had, a, listen, last, night, As, You, Ble...  Positive\n",
       "3                                         (CONGRATS)  Positive\n",
       "4  (yeaaaah, yippppy, !, !, !, my, accnt, verifie...  Positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove mentions, hashtags, and links\n",
    "def text_cleaner(text):\n",
    "    clean_text = []\n",
    "    for tweet in text:\n",
    "        tweet = re.sub('@\\S+', '', tweet)\n",
    "        tweet = re.sub('http\\S+', '', tweet)\n",
    "        tweet = re.sub('#\\S+', '', tweet)\n",
    "        tweet = re.sub('\\.+', '', tweet)\n",
    "        tweet = re.sub(':\\S+', '', tweet)\n",
    "        tweet = ' '.join(tweet.split())\n",
    "        clean_text.append(tweet)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "clean_pos = text_cleaner(pos[0:1000])\n",
    "clean_neg = text_cleaner(neg[0:1000])\n",
    "\n",
    "#pos_tweet = [[tweet, 'Positive'] for tweet in clean_pos]\n",
    "#neg_tweet = [[tweet, 'Positive'] for tweet in clean_neg]\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "pos_string = \" \".join(tweet for tweet in clean_pos)\n",
    "neg_string = \" \".join(tweet for tweet in clean_neg)\n",
    "\n",
    "pos_str_doc = nlp(pos_string)\n",
    "neg_str_doc = nlp(neg_string)\n",
    "\n",
    "pos_doc = [[nlp(tweet), \"Positive\"] for tweet in clean_pos]\n",
    "neg_doc = [[nlp(tweet), \"Negative\"] for tweet in clean_neg]\n",
    "\n",
    "tweet_df = pd.DataFrame(pos_doc + neg_doc)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n",
      "Processing row 1900\n",
      "Processing row 1950\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finger</th>\n",
       "      <th>work</th>\n",
       "      <th>dry</th>\n",
       "      <th>bisexuality</th>\n",
       "      <th>ass</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>eng</th>\n",
       "      <th>phantasy</th>\n",
       "      <th>em</th>\n",
       "      <th>nike</th>\n",
       "      <th>...</th>\n",
       "      <th>touch</th>\n",
       "      <th>hat</th>\n",
       "      <th>till</th>\n",
       "      <th>al</th>\n",
       "      <th>yung</th>\n",
       "      <th>properly</th>\n",
       "      <th>slot</th>\n",
       "      <th>full</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(for, being, top, engaged, members, in, my, co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Hey, James, !, How, odd, Please, call, our, C...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(we, had, a, listen, last, night, As, You, Ble...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(CONGRATS)</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(yeaaaah, yippppy, !, !, !, my, accnt, verifie...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  finger work dry bisexuality ass accumulate eng phantasy em nike  ... touch  \\\n",
       "0      0    0   0           0   0          0   0        0  0    0  ...     0   \n",
       "1      0    0   0           0   0          0   0        0  0    0  ...     0   \n",
       "2      0    0   0           0   0          0   0        0  0    0  ...     0   \n",
       "3      0    0   0           0   0          0   0        0  0    0  ...     0   \n",
       "4      0    0   0           0   0          0   0        0  0    0  ...     0   \n",
       "\n",
       "  hat till al yung properly slot full  \\\n",
       "0   0    0  0    0        0    0    0   \n",
       "1   0    0  0    0        0    0    0   \n",
       "2   0    0  0    0        0    0    0   \n",
       "3   0    0  0    0        0    0    0   \n",
       "4   0    0  0    0        0    0    0   \n",
       "\n",
       "                                       text_sentence text_sources  \n",
       "0  (for, being, top, engaged, members, in, my, co...     Positive  \n",
       "1  (Hey, James, !, How, odd, Please, call, our, C...     Positive  \n",
       "2  (we, had, a, listen, last, night, As, You, Ble...     Positive  \n",
       "3                                         (CONGRATS)     Positive  \n",
       "4  (yeaaaah, yippppy, !, !, !, my, accnt, verifie...     Positive  \n",
       "\n",
       "[5 rows x 3270 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features using two different NLP methods: BoW \n",
    "\n",
    "# Utility function to define the 2000 most common words\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter punctionation and stopwords\n",
    "    allwords = [token.lemma_.lower() for token in text \n",
    "                if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    # Return 2000 most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "# Create DF with features\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the DF and initialize counts to 0\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_sources'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurance of words in each sentence\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert to lemmas, filter out punctuation, stopwords and uncommon words \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words)]\n",
    "        \n",
    "        # Populate rows with word counts\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # Checks for hung kernel\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Set up bags\n",
    "pos_words = bag_of_words(pos_str_doc)\n",
    "neg_words = bag_of_words(neg_str_doc)\n",
    "\n",
    "# Combine two bags to create a set of unique words\n",
    "common_words = set(pos_words + neg_words)\n",
    "\n",
    "# Create dataset with features. Computationally intensive\n",
    "word_counts = bow_features(tweet_df, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkovach/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Score: 0.9591666666666666\n",
      "Test Set Score: 0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkovach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Score: 0.9191666666666667\n",
      "Test Set Score: 0.695\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_sources</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Negative</td>\n",
       "      <td>306</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Positive</td>\n",
       "      <td>161</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0         Negative  Positive\n",
       "text_sources                    \n",
       "Negative           306        83\n",
       "Positive           161       250"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "\n",
    "# BoW with random forest\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_sources']\n",
    "X = np.array(word_counts.drop(['text_sentence', 'text_sources'], 1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4,\n",
    "                                                   random_state=0)\n",
    "\n",
    "train = rfc.fit(X_train, Y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "print(\"Training Set Score:\", rfc.score(X_train, Y_train))\n",
    "print(\"Test Set Score:\", rfc.score(X_test, Y_test))\n",
    "pd.crosstab(Y_test, rfc_pred)\n",
    "\n",
    "# Bag of Words with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, Y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(\"Training Set Score:\", lr.score(X_train, Y_train))\n",
    "print(\"Test Set Score:\", lr.score(X_test, Y_test))\n",
    "pd.crosstab(Y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1061\n",
      "Original Sentence: thank you eonnie\n",
      "TF-IDF Vector: {'thank': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Create features using two different NLP methods: tf-idf.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clean_tweet = clean_pos + clean_neg\n",
    "\n",
    "X_train, X_test = train_test_split(clean_tweet, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, #only use words that appear at least twice\n",
    "                             stop_words='english',\n",
    "                             lowercase=True, #converts all words to lowercase\n",
    "                             use_idf=True, #use idf as weights\n",
    "                             norm=u'l1', #correction factor for document length (L1, L2, or max)\n",
    "                             smooth_idf=True #Adds 1 to all doc frequencies, prevents divide by zero errors\n",
    "                            )\n",
    "\n",
    "# Apply vectorizer and split to training/test sets\n",
    "tweet_tfidf = vectorizer.fit_transform(clean_tweet)\n",
    "print(\"Number of features: %d\" % tweet_tfidf.get_shape()[1])\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(tweet_tfidf, test_size=0.4, random_state=0)\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "# Get number of tweets\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "tfidf_bytweet = [{} for _ in range(0,n)]\n",
    "\n",
    "# Feature list\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# List feature words and tfidf scores by tweet\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bytweet[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "print(\"Original Sentence:\", X_train[0])\n",
    "print(\"TF-IDF Vector:\", tfidf_bytweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by new components: 60.31351699537344\n",
      "Component 0\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "follow &amp; follow u back    0.99333\n",
      "Name: 0, dtype: float64\n",
      "Component 1\n",
      "heyy i miss you                     0.996829\n",
      "I miss his massages                 0.996829\n",
      "miss you so much xxxxxx             0.996829\n",
      "i miss them so much                 0.996829\n",
      "miss you                            0.996829\n",
      "Miss chillin'with you               0.996829\n",
      "miss you                            0.996829\n",
      "i miss you                          0.996829\n",
      "Already miss so much                0.996829\n",
      "French mixers miss you so much 💜    0.996349\n",
      "Name: 1, dtype: float64\n",
      "Component 2\n",
      "thank you eonnie                                                                                         0.997597\n",
      "Thank you                                                                                                0.997597\n",
      "Thank you : )                                                                                            0.997597\n",
      "Thank you Beiruting                                                                                      0.997597\n",
      "Thank you!                                                                                               0.997597\n",
      "thank you                                                                                                0.997597\n",
      "thank you!!                                                                                              0.997597\n",
      "Thank you                                                                                                0.997597\n",
      "Yesss I am thank you                                                                                     0.997009\n",
      "Benson is gorgeous!! Thank you so much for sharing him with us You can also enter on our Facebook too    0.994939\n",
      "Name: 2, dtype: float64\n",
      "Component 3\n",
      "It is indeed - many thanks                     0.996242\n",
      "thanks                                         0.996242\n",
      "thanks                                         0.996242\n",
      "done thanks                                    0.996242\n",
      "thanks!!!                                      0.996242\n",
      "Thanks Kalin                                   0.996242\n",
      "thanks                                         0.996242\n",
      "Thanks for updating your profile page          0.995431\n",
      "It has been fixed, thanks for your feedback    0.995321\n",
      "Visit my blog thanks                           0.995207\n",
      "Name: 3, dtype: float64\n",
      "Component 4\n",
      "I love u                                                        0.987644\n",
      "love u 2                                                        0.987644\n",
      "I love you                                                      0.987644\n",
      "i love you                                                      0.987644\n",
      "I love hozier                                                   0.987644\n",
      "gotta love timezones                                            0.985869\n",
      "I love blue                                                     0.985855\n",
      "Who Wouldn't Love These BigJuicySelfies -                       0.984889\n",
      "love yousad I wasn't up front                                   0.984221\n",
      "\"fleekile mbalula\", he said i repeat, i love my south africa    0.971573\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Capture components with SVD\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# SVD data reducer, reducing to 130 features\n",
    "svd = TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data then project\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "exp_var = svd.explained_variance_ratio_\n",
    "total_variance = exp_var.sum()\n",
    "print(\"Variance captured by new components: {}\".format(total_variance*100))\n",
    "\n",
    "# See what paragraphs the model considers similar\n",
    "paras_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
